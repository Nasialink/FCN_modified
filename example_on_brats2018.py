# -*- coding: utf-8 -*-
"""Example_on_BRATS2018.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1roDtATRJph4xDJ2ODgen4RIF5zpbDvsb

<a href="https://colab.research.google.com/github/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/blob/master/Example_on_BRATS2018.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

Note
- [Colab Only] Means the associated cell is required only if you are running the model on Google Colaboratory

## Mount Drive [Colab Only]
Mount the google drive to access the dataset stored on drive.


from google.colab import drive
drive.mount('/gdrive')"""

"""## Extract the Dataset"""
import os
import tensorflow
import keras
import math
import matplotlib.pyplot as plt
import zipfile  # For faster extraction
import SimpleITK as sitk  # For loading the dataset
import numpy as np  # For data manipulation
from model import build_model  # For creating the model
import glob  # For populating the list of files
from scipy.ndimage import zoom  # For resizing
import re  # For parsing the filenames (to know their modality)
from datetime import datetime
from PIL import Image
#import cv2
from functions import *
from sklearn.model_selection import train_test_split
import json 
import datetime
import nibabel as nib
tensorflow.compat.v1.disable_eager_execution() #my addition

"""dataset_path = "/content/gdrive/MyDrive/Diplomatiki_new/brats/MICCAI_BraTS_2018_Data_Training.zip"  # Replace with your dataset path
##dataset_path = "/gdrive/MyDrive/Diplomatiki_new/MICCAI_BraTS_2018_Data_Training.zip"
zfile = zipfile.ZipFile(dataset_path)
zfile.extractall()
print(dataset_path)
print(zfile)"""
"""## Get required packages
- **SimpleITK**: For loading the dataset
- **[model.py](https://github.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/)**: The model from BRATS2018 winning paper
"""

"""!pip install simpleitk
!wget https://github.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/raw/master/model.py"""

"""## Imports and helper functions"""



"""## Loading Data

"""
data_path = '/home/nasia/Documents/BRATS/data/MICCAI_BraTS_2018_Data_Training'

path_checkpoint = '/home/nasia/Documents/BRATS/data/training/cp.ckpt'
save_pred_path = '/home/nasia/Documents/BRATS/data/predictions'
save_model_path = '/home/nasia/Documents/BRATS/data/saved_model'
test_idx_file = '/home/nasia/Documents/BRATS/data/testset'+datetime.datetime.now()+'.json'
count = 20
batch_size = 1
epochs = 50
# Get a list of files for all modalities individually
t1 = glob.glob(data_path+'/*GG/*/*t1.nii.gz')

t2 = glob.glob(data_path+'/*GG/*/*t2.nii.gz')
flair = glob.glob(data_path+'/*GG/*/*flair.nii.gz')
t1ce = glob.glob(data_path+'/*GG/*/*t1ce.nii.gz')
seg = glob.glob(data_path+'/*GG/*/*seg.nii.gz')  # Ground Truth

"""Parse all the filenames and create a dictionary for each patient with structure:

{<br />
    &nbsp;&nbsp;&nbsp;&nbsp;'t1': _<path to t1 MRI file&gt;_,<br />
    &nbsp;&nbsp;&nbsp;&nbsp;'t2': _<path to t2 MRI&gt;_,<br />
    &nbsp;&nbsp;&nbsp;&nbsp;'flair': _<path to FLAIR MRI file&gt;_,<br />
    &nbsp;&nbsp;&nbsp;&nbsp;'t1ce': _<path to t1ce MRI file&gt;_,<br />
    &nbsp;&nbsp;&nbsp;&nbsp;'seg': _<path to Ground Truth file&gt;_,<br />
}<br />
"""

pat = re.compile('.*_(\w*)\.nii\.gz')

data_paths = [{
    pat.findall(item)[0]:item
    for item in items
}
for items in list(zip(t1, t2, t1ce, flair, seg))]

#print(data_paths[5:10])

"""## Load the data in a Numpy array
Creating an empty Numpy array beforehand and then filling up the data helps you gauge beforehand if the data fits in your memory.

_Loading only the first 4 images here, to save time._
"""

input_shape = (4, 80, 96, 64)
output_channels = 3
data = np.empty((len(data_paths[:count]),) + input_shape, dtype=np.float32)
labels = np.empty((len(data_paths[:count]), output_channels) + input_shape[1:], dtype=np.uint8)

#data = np.empty((len(data_paths[:2]),) + input_shape, dtype=np.float32)
#labels = np.empty((len(data_paths[:2]), output_channels) + input_shape[1:], dtype=np.uint8)
img = nib.load(example_filename)
"""from pprint import pprint

pprint(globals())
pprint(locals())"""
# Parameters for the progress bar
# total = len(data_paths[:count])
# #total = data.shape[0]
# step = 25 / total

# for i, imgs in enumerate(data_paths[:count]):
# #for i, imgs in enumerate(data_paths[:2]):
#     try:
#         data[i] = np.array([preprocess(read_img(imgs[m]), input_shape[1:]) for m in ['t1', 't2', 't1ce', 'flair']], dtype=np.float32)
#         labels[i] = preprocess_label(read_img(imgs['seg']), input_shape[1:])[None, ...]
        
#         # Print the progress bar
#         print('\r' + f'Progress: '
#             f"[{'=' * int((i+1) * step) + ' ' * (24 - int((i+1) * step))}]"
#             f"({math.ceil((i+1) * 100 / (total))} %)",
#             end='')
#     except Exception as e:
#         print(f'Something went wrong with {imgs["t1"]}, skipping...\n Exception:\n{str(e)}')
#         continue

# Split into training and test set 
data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.25, random_state=None)
print(data_train.shape, data_test.shape, labels_train.shape, labels_test.shape)
print(np.max(labels_train))
# data = data[data_train]
# labels = labels[labels_train]
testset = {'data_test': data_test, 'labels_test': labels_test}
with open(test_idx_file, 'w') as f:
    f.write(json.dump(testset), indent=4)

## Model

#build the model


model = build_model(input_shape=input_shape, output_channels=3)

#model.summary()

directory_checkpoint = os.path.dirname(path_checkpoint)

#cp_callback = tensorflow.keras.callbacks.ModelCheckpoint(filepath=path_checkpoint,save_weights_only=True,verbose=1)
cp_callback = keras.callbacks.ModelCheckpoint(filepath=path_checkpoint,save_weights_only=True,verbose=1)

"""Train the model"""

model.fit(data, [labels, data], batch_size=batch_size, epochs=epochs, callbacks=[cp_callback])
#model.fit(data, [labels, data], batch_size=1, epochs=1)
"""That's it!"""

"""Extra additions for printing the NN architecture

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='/content/gdrive/MyDrive/Diplomatiki_new/brats/model_plot.png', show_shapes=True, show_layer_names=True)"""

#model.load_weights(path_checkpoint)
os.makedirs('saved_model', exist_ok=True)
model.save(os.path.join(save_model_path,'mymodel.keras'))



# input_shape = (4, 80, 96, 64)
# output_channels = 3
# data_v = np.empty((len(data_paths[5:6]),) + input_shape, dtype=np.float32)
# labels_v = np.empty((len(data_paths[5:6]), output_channels) + input_shape[1:], dtype=np.uint8)

# total_v = len(data_paths[284:285])
# step_v = 25 / total

# for i, imgs in enumerate(data_paths[284:285]):
#     try:
#         data_v[i] = np.array([preprocess(read_img(imgs[m]), input_shape[1:]) for m in ['t1', 't2', 't1ce', 'flair']], dtype=np.float32)
#         labels_v[i] = preprocess_label(read_img(imgs['seg']), input_shape[1:])[None, ...]
        
#         # Print the progress bar
#         print('\r' + f'Progress: '
#             f"[{'=' * int((i+1) * step) + ' ' * (24 - int((i+1) * step_v))}]"
#             f"({math.ceil((i+1) * 100 / (total_v))} %)",
#             end='')
#     except Exception as e:
#         print(f'Something went wrong with {imgs["t1"]}, skipping...\n Exception:\n{str(e)}')
#         continue

    
# def predict_gt(model, input_data):
#     # Set the VAE branch's output layer to None
#     model.get_layer('Dec_VAE_Output').outbound_nodes = []

#     # Predict using the modified model
#     predictions = model.predict(input_data)

#     # Return only the predictions from the GT branch
#     gt_predictions = predictions[0]

#     return gt_predictions

# y_pred= predict_gt(model, data_v)
# print(type(y_pred), y_pred.shape)
# np.save(save_pred_path, y_pred)

"""for i, arr in enumerate(y_pred):
    print(f"Array {i+1} shape: {arr.shape}")"""
#y=np.array([np.array(xi) for xi in y_pred])
#y = np.stack(y_pred, axis=0)
#print(type(y))
#y_pred=np.array(y_pred)
#np.savetxt('/content/gdrive/MyDrive/Diplomatiki_new/y_pred.txt', y_pred)
#print("it is:", type(y_pred), len(y_pred))
#print(y_pred)
#print("it is:", type(data_v), data_v.shape)
